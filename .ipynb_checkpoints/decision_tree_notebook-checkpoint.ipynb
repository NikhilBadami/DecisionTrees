{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 6601 Assignment 4: Decision Tree Learning\n",
    "\n",
    "Machine learning offers a number of methods for classifying data into discrete categories, such as k-means clustering. Decision trees provide a structure for such categorization, based on a series of decisions that led to separate distinct outcomes. In this assignment, you will work with decision trees to perform binary classification according to some decision boundary.  Your challenge is to build and to train decision trees capable of solving useful classification problems. You will learn first how to build decision trees, then how to effectively train them and finally how to test their performance.\n",
    "\n",
    "This assignment is due on T-Square in decision_trees.py on March 19th midnight AOE (anywhere on Earth).\n",
    "\n",
    "<img src=\"files/dt.png\" width=\"50%\" align=\"middle\">\n",
    "\n",
    "Abstract\n",
    "-------\n",
    "You will build, train and test decision tree models to perform basic classification tasks.\n",
    "\n",
    "Motivation\n",
    "-------\n",
    "Classification is used widely in machine learning to figure out how to sort new data that comes through.\n",
    "\n",
    "Objectives\n",
    "-------\n",
    "Students should understand how decision trees and random forests work.\n",
    "Students should develop and intuition for how and why accuracy differs for training and testing data based on different parameters.\n",
    "\n",
    "Evaluation\n",
    "-------\n",
    "Evaluation is using the last submission on Bonnie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "-------\n",
    "\n",
    "For this assignment we're going to need an explicit way to make structured decisions. The following is `DecisionNode`,  representing a decision node as some atomic choice in a binary decision graph. It can represent a class label (i.e. a final decision) or a binary decision to guide the us through a flow-chart to arrive at a decision. Note that in this representation 'True' values for a decision take us to the left. This is arbitrary but matters for what comes next.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    \"\"\"Class to represent a single node in\n",
    "    a decision tree.\"\"\"\n",
    "\n",
    "    def __init__(self, left, right, decision_function,class_label=None):\n",
    "        \"\"\"Create a node with a left child, right child,\n",
    "        decision function and optional class label\n",
    "        for leaf nodes.\"\"\"\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.decision_function = decision_function\n",
    "        self.class_label = class_label\n",
    "\n",
    "    def decide(self, feature):\n",
    "        \"\"\"Return on a label if node is leaf,\n",
    "        or pass the decision down to the node's\n",
    "        left/right child (depending on decision\n",
    "        function).\"\"\"\n",
    "        if self.class_label is not None:\n",
    "            return self.class_label\n",
    "        elif self.decision_function(feature):\n",
    "            return self.left.decide(feature)\n",
    "        else:\n",
    "            return self.right.decide(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1a: Building a Binary Tree by Hand\n",
    "--------\n",
    "15 pts.\n",
    "\n",
    "In `build_decision_tree()`, construct a tree of decision nodes by hand in order to classify the data below, i.e. map each datum $x$ to a label $y$. Select tests to be as small as possible (in terms of attributes), breaking ties among tests with the same number of attributes by selecting the one that classifies the greatest number of examples correctly. If multiple tests have the same number of attributes and classify the same number of examples, then break the tie using attributes with lower index numbers (e.g. select $A_1$ over $A_2$)\n",
    "\n",
    "| Datum  | $A_1$ | $A_2$ | $A_3$ | $A_4$ |  y  |\n",
    "| -------| :---: | :---: | :---: | :---: | ---:|\n",
    "| $x_1$  |   1   |   0   |   0   |   0   |  1  |\n",
    "| $x_2$  |   1   |   0   |   1   |   1   |  1  |\n",
    "| $x_3$  |   0   |   1   |   0   |   0   |  1  |\n",
    "| $x_4$  |   0   |   1   |   1   |   0   |  0  |\n",
    "| $x_5$  |   1   |   1   |   0   |   1   |  1  |\n",
    "| $x_6$  |   0   |   1   |   0   |   1   |  0  |\n",
    "| $x_7$  |   0   |   0   |   1   |   1   |  1  |\n",
    "| $x_8$  |   0   |   0   |   1   |   0   |  0  |\n",
    "\n",
    "Hints: \n",
    "To get started, it might help to draw out the tree by hand with each attribute representing a node.\n",
    "\n",
    "To create the decision function to pass to your `DecisionNode`, you can create a lambda expression as follows:\n",
    "\n",
    "    func = lambda feature : feature[2] == 0\n",
    "    \n",
    "in which we would choose the left node if the third attribute is 0.\n",
    "\n",
    "The tree nodes should be less than 10 nodes including the leaf (not the number of instances, but the actual nodes in the tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examples = [[1,0,0,0],\n",
    "            [1,0,1,1],\n",
    "            [0,1,0,0],\n",
    "            [0,1,1,0],\n",
    "            [1,1,0,1],\n",
    "            [0,1,0,1],\n",
    "            [0,0,1,1],\n",
    "            [0,0,1,0]]\n",
    "\n",
    "classes = [1,1,1,0,1,0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_decision_tree():\n",
    "    \"\"\"Create decision tree\n",
    "    capable of handling the provided \n",
    "    data.\"\"\"\n",
    "    # TODO: build full tree from root\n",
    "    decision_tree_root = None\n",
    "    \n",
    "    return decision_tree_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decision_tree_root = build_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1b: Validation\n",
    "--------\n",
    "5 pts.\n",
    "\n",
    "Now that we have a decision tree, we're going to need some way to evaluate its performance. In most cases we'd reserve a portion of the training data for evaluation, or use [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). For now let's just see how your tree does on the provided examples. In the stubbed out code below, fill out the methods to compute accuracy, precision, recall, and the confusion matrix for your classifier output. `classifier_output` is just the list of labels that your classifier outputs, corresponding to the same examples as `true_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(classifier_output, true_labels):\n",
    "    #TODO output should be [[true_positive, false_negative], [false_positive, true_negative]]\n",
    "    raise NotImplemented()\n",
    "\n",
    "def precision(classifier_output, true_labels):\n",
    "    #TODO precision is measured as: true_positive/ (true_positive + false_positive)\n",
    "    raise NotImplemented()\n",
    "    \n",
    "def recall(classifier_output, true_labels):\n",
    "    #TODO: recall is measured as: true_positive/ (true_positive + false_negative)\n",
    "    raise NotImplemented()\n",
    "    \n",
    "def accuracy(classifier_output, true_labels):\n",
    "    #TODO accuracy is measured as:  correct_classifications / total_number_examples\n",
    "    raise NotImplemented()\n",
    "    \n",
    "classifier_output = [decision_tree_root.decide(example) for example in examples]\n",
    "\n",
    "p1_accuracy = accuracy( classifier_output, classes )\n",
    "p1_precision = precision(classifier_output, classes)\n",
    "p1_recall = recall(classifier_output, classes)\n",
    "p1_confusion_matrix = confusion_matrix(classifier_output, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2a: Decision Tree Learning\n",
    "-------\n",
    "30 pts.\n",
    "\n",
    "File to use: part2_data.csv\n",
    "\n",
    "\n",
    "As the size of our training set grows, it rapidly becomes impractical to build these trees by hand. We need a procedure to automagically construct these trees.\n",
    "\n",
    "For starters, let's consider the following algorithm (a variation of [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm)) for the construction of a decision tree from a given set of examples:\n",
    "\n",
    "    1) Check for base cases: \n",
    "         a) If all elements of a list are of the same class, return a leaf node with the appropriate class label.\n",
    "         b) If a specified depth limit is reached, return a leaf labeled with the most frequent class.\n",
    "\n",
    "    2) For each attribute alpha: evaluate the normalized information gain gained by splitting on attribute $\\alpha$\n",
    "\n",
    "    3) Let alpha_best be the attribute with the highest normalized information gain\n",
    "\n",
    "    4) Create a decision node that splits on alpha_best\n",
    "\n",
    "    5) Recur on the sublists obtained by splitting on alpha_best, and add those nodes as children of node\n",
    "\n",
    "First, in the `DecisionTree.__build_tree__()` method implement the above algorithm. You will need to implement `entropy()` and `information_gain()` in order to do so (hints [here](https://en.wikipedia.org/wiki/Entropy_(information_theory)) and [here](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)).\n",
    "\n",
    "Next, in `DecisionTree.classify()` below, write a function to produce classifications for a list of features once your decision tree has been built.\n",
    "\n",
    "Some other helpful notes:\n",
    "\n",
    "    1) Your features and classify should be in numpy arrays where if the dataset was (m x n) then the features is (m x n-1) and classify is (m x 1).\n",
    "\n",
    "    2) You need at least an average accuracy of 60% to get full credit for this part.\n",
    "\n",
    "    3) These features are continuous features and you will need to split based on a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NotImplementedType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f0a642eb5158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minformation_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0.08\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"TEST FAILED\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mtest_information_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mtest_entropy_infogain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f0a642eb5158>\u001b[0m in \u001b[0;36mtest_information_gain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m    \u001b[0;31m# split_food_type = [np.array(i) for i in split_food_type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m    \u001b[0mgain_patrons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minformation_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestaurants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_patrons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m    \u001b[0mgain_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minformation_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestaurants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_food_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m    \u001b[0;32massert\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgain_patrons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.541\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Information Gain on patrons should be 0.541\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f0a642eb5158>\u001b[0m in \u001b[0;36minformation_gain\u001b[0;34m(previous_classes, current_classes)\u001b[0m\n\u001b[1;32m     10\u001b[0m     lists where each list has 0 and 1 values).\"\"\"\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# TODO: finish this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NotImplementedType' object is not callable"
     ]
    }
   ],
   "source": [
    "def entropy(class_vector):\n",
    "    \"\"\"Compute the entropy for a list\n",
    "    of classes (given as either 0 or 1).\"\"\"\n",
    "    # TODO: finish this\n",
    "    raise NotImplemented()\n",
    "    \n",
    "def information_gain(previous_classes, current_classes ):\n",
    "    \"\"\"Compute the information gain between the\n",
    "    previous and current classes (a list of \n",
    "    lists where each list has 0 and 1 values).\"\"\"\n",
    "    # TODO: finish this\n",
    "    raise NotImplemented()\n",
    "\n",
    "class DecisionTree():\n",
    "    \"\"\"Class for automatic tree-building\n",
    "    and classification.\"\"\"\n",
    "\n",
    "    def __init__(self, depth_limit=float('inf')):\n",
    "        \"\"\"Create a decision tree with an empty root\n",
    "        and the specified depth limit.\"\"\"\n",
    "        self.root = None\n",
    "        self.depth_limit = depth_limit\n",
    "\n",
    "    def fit(self, features, classes):\n",
    "        \"\"\"Build the tree from root using __build_tree__().\"\"\"\n",
    "        self.root = self.__build_tree__(features, classes)\n",
    "\n",
    "    def __build_tree__(self, features, classes, depth=0):  \n",
    "        \"\"\"Implement the above algorithm to build\n",
    "        the decision tree using the given features and\n",
    "        classes to build the decision functions.\"\"\"\n",
    "        #TODO: finish this\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def classify(self, features):\n",
    "        \"\"\"Use the fitted tree to \n",
    "        classify a list of examples. \n",
    "        Return a list of class labels.\"\"\"\n",
    "        class_labels = []\n",
    "        #TODO: finish this\n",
    "        raise NotImplemented()\n",
    "        return class_labels\n",
    "    \n",
    "def test_information_gain():\n",
    "   \"\"\" Assumes information_gain() accepts (classes, [list of subclasses])\n",
    "       Feel free to edit / enhance this note with more tests \"\"\"\n",
    "   restaurants = [0]*6 + [1]*6\n",
    "   split_patrons =   [[0,0], [1,1,1,1], [1,1,0,0,0,0]]\n",
    "   split_food_type = [[0,1],[0,1],[0,0,1,1],[0,0,1,1]]\n",
    "    \n",
    "   # If you're using numpy indexing add the following before calling information_gain()\n",
    "   # split_patrons =   [np.array(i) for i in split_patrons]   #convert to np array \n",
    "   # split_food_type = [np.array(i) for i in split_food_type]\n",
    "\n",
    "   gain_patrons = information_gain(restaurants, split_patrons)\n",
    "   gain_type = information_gain(restaurants, split_food_type)\n",
    "   assert round(gain_patrons,3) == 0.541, \"Information Gain on patrons should be 0.541\"\n",
    "   assert gain_type == 0.0, \"Information gain on type should be 0.0\"\n",
    "   print \"Information Gain calculations correct...\"\n",
    "\n",
    "def test_entropy_infogain(): \n",
    "    assert (entropy([1,1,1,0,0,0])==1),\"TEST FAILED\"\n",
    "    assert (entropy([1,1,1,1,1,1])==0),\"TEST FAILED\"\n",
    "    assert (int(entropy([1,1,0,0,0,0])*100)==91),\"TEST FAILED\"\n",
    "    assert (information_gain([1,1,1,0,0,0],[[1,1,1],[0,0,0]])==1),\"TEST FAILED\"\n",
    "    assert (round(information_gain([1,1,1,0,0,0],[[1,1,0],[1,0,0]]),2)==0.08),\"TEST FAILED\"\n",
    "    \n",
    "test_information_gain()\n",
    "test_entropy_infogain() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/infogain.png\" width=\"50%\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2b: Validation\n",
    "--------\n",
    "10 pts.\n",
    "\n",
    "For this part of the assignment we're going to use a relatively simple dataset (banknote authentication, found in `part_2_data.csv`). In the section below there are methods to load the data in a consistent format.\n",
    "\n",
    "In general, reserving part of your data as a test set can lead to unpredictable performance- a serendipitous choice of your train or test split could give you a very inaccurate idea of how your classifier performs. That's where k-fold cross validation comes in.\n",
    "\n",
    "In `generate_k_folds()`, we'll split the dataset at random into k equal subsections. Then iterating on each of our k samples, we'll reserve that sample for testing and use the other k-1 for training. Averaging the results of each fold should give us a more consistent idea of how the classifier is doing across the data as a whole.\n",
    "\n",
    "You should get at least an average accuracy of 60% for ten_folds to get full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv(data_file_path, class_index=-1):\n",
    "    handle = open(data_file_path, 'r')\n",
    "    contents = handle.read()\n",
    "    handle.close()\n",
    "    rows = contents.split('\\n')\n",
    "    out = np.array([[float(i) for i in r.split(',')] for r in rows if r ])\n",
    "    classes= map(int,  out[:, class_index])\n",
    "    features = out[:, :class_index]\n",
    "    return features, classes\n",
    "\n",
    "def generate_k_folds(dataset, k):\n",
    "    #TODO this method should return a list of folds,\n",
    "    # where each fold is a tuple like (training_set, test_set)\n",
    "    # where each set is a tuple like (examples, classes)\n",
    "    raise NotImplemented()\n",
    "    \n",
    "dataset = load_csv('part2_data.csv')\n",
    "ten_folds = generate_k_folds(dataset, 10)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "confusion = []\n",
    "\n",
    "for fold in ten_folds:\n",
    "    train, test = fold\n",
    "    train_features, train_classes = train\n",
    "    test_features, test_classes = test\n",
    "    tree = DecisionTree( )\n",
    "    tree.fit( train_features, train_classes)\n",
    "    output = tree.classify(test_features)\n",
    "    \n",
    "    accuracies.append( accuracy(output, test_classes))\n",
    "    precisions.append( precision(output, test_classes))\n",
    "    recalls.append( recall(output, test_classes))\n",
    "    confusion.append( confusion_matrix(output, test_classes))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Random Forests\n",
    "-------\n",
    "30 pts.\n",
    "\n",
    "File to use: part2_data.csv\n",
    "\n",
    "\n",
    "The decision boundaries drawn by decision trees are very sharp, and fitting a decision tree of unbounded depth to a list of training examples almost inevitably leads to overfitting. In an attempt to decrease the variance of our classifier we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated 'bagging').\n",
    "\n",
    "A Random Forest is a collection of decision trees, build as follows:\n",
    "\n",
    "    1) For every tree we're going to build:\n",
    "\n",
    "        a) Subsample the examples provided us (with replacement) in accordance with a provided example subsampling rate.\n",
    "\n",
    "        b) From the sample in a), choose attributes at random to learn on (in accordance with a provided attribute subsampling rate)\n",
    "\n",
    "        c) Fit a decision tree to the subsample of data we've chosen (to a certain depth)\n",
    "    \n",
    "Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n",
    "Fill in `RandomForest.fit()` to fit the decision tree as we describe above, and fill in `RandomForest.classify()` to classify a given list of examples.\n",
    "\n",
    "Your features and classify should be in numpy arrays where if the dataset was (m x n) then the features is (m x n-1) and classify is (n x 1).\n",
    "\n",
    "You need at least an average accuracy of 75% to get full credit for this part. To test, we will be using a forest with 5 trees, with a depth limit of 5, example subsample rate of 0.5 and attribute subsample rate of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \"\"\"Class for random forest\n",
    "    classification.\"\"\"\n",
    "\n",
    "    def __init__(self, num_trees, depth_limit, example_subsample_rate, attr_subsample_rate):\n",
    "        \"\"\"Create a random forest with a fixed \n",
    "        number of trees, depth limit, example\n",
    "        sub-sample rate and attribute sub-sample\n",
    "        rate.\"\"\"\n",
    "        self.trees = []\n",
    "        self.num_trees = num_trees\n",
    "        self.depth_limit = depth_limit\n",
    "        self.example_subsample_rate = example_subsample_rate\n",
    "        self.attr_subsample_rate = attr_subsample_rate\n",
    "\n",
    "    def fit(self, features, classes):\n",
    "        \"\"\"Build a random forest of \n",
    "        decision trees.\"\"\"\n",
    "        # TODO implement the above algorithm\n",
    "        raise NotImplemented()\n",
    "\n",
    "    def classify(self, features):\n",
    "        \"\"\"Classify a list of features based\n",
    "        on the trained random forest.\"\"\"\n",
    "        # TODO implement classification for a random forest.\n",
    "        raise NotImplemented()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Challenge Classifier\n",
    "-------\n",
    "\n",
    "10 points.\n",
    "\n",
    "File to use: challenge_data.pickle\n",
    "\n",
    "You've been provided with a sample of data from a research dataset in 'challenge_data.pickle'. It is serialized as a tuple of (features, classes) and you will need to create a tree structure. I have reserved a part of the dataset for testing called challenge_test (which you do not have access to). \n",
    "\n",
    "To get full points for this part of the assignment, you'll need to get at least an average accuracy of 80% on the training data you have (challenge_data.pickle), and at least an average accuracy of 60% on the holdout/test set (challenge_test).\n",
    "\n",
    "We will also be having a competition using your challenge classifier. The classifier that performs most accurately on the holdout/test set wins (so optimize for accuracy). You will not be provided with the holdout set. Ties will be broken by submission time.\n",
    "\n",
    "<ul>\n",
    "    <li>First place:  3 bonus points on your final grade </li>\n",
    "    <li>Second place: 2 bonus points on your final grade </li>\n",
    "    <li>Third place:  1 bonus point on your final grade </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ChallengeClassifier():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # initialize whatever parameters you may need here-\n",
    "        # this method will be called without parameters \n",
    "        # so if you add any to make parameter sweeps easier, provide defaults\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def fit(self, features, classes):\n",
    "        # fit your model to the provided features\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def classify(self, features):\n",
    "        # classify each feature in features as either 0 or 1.\n",
    "        raise NotImplemented()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
