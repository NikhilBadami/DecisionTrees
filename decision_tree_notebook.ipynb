{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Decision Tree Learning\n",
    "\n",
    "In this assignment, you will work with a class of reinforcement learning agents called decision trees to perform binary classification according to some decision boundary. You will learn first how to build decision trees, then how to effectively train them and finally how to test their performance.\n",
    "\n",
    "\n",
    "This assignment is due on T-Square in decision_trees.py on November 6th midnight AOE (anywhere on Earth)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "-------\n",
    "\n",
    "For this assignment we're going to need an explicit way to make structured decisions. The following is `DecisionNode`, representing a decision node as some atomic choice in a binary decision graph. It can represent a class label (i.e. a final decision) or a binary decision to guide the us through a flow-chart to arrive at a decision. Note that in this representation 'True' values for a decision take us to the left. This is arbitrary but matters for what comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    \"\"\"Class to represent a single node in\n",
    "    a decision tree.\"\"\"\n",
    "\n",
    "    def __init__(self, left, right, decision_function,class_label=None):\n",
    "        \"\"\"Create a node with a left child, right child,\n",
    "        decision function and optional class label\n",
    "        for leaf nodes.\"\"\"\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.decision_function = decision_function\n",
    "        self.class_label = class_label\n",
    "\n",
    "    def decide(self, feature):\n",
    "        \"\"\"Return on a label if node is leaf,\n",
    "        or pass the decision down to the node's\n",
    "        left/right child (depending on decision\n",
    "        function).\"\"\"\n",
    "        if self.class_label:\n",
    "            return self.class_label\n",
    "        elif self.decision_function(feature):\n",
    "            return self.left.decide(feature)\n",
    "        else:\n",
    "            return self.right.decide(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1a: Building a binary tree by hand\n",
    "--------\n",
    "15 pts.\n",
    "\n",
    "In `build_decision_tree()`, construct a tree of decision nodes by hand in order to classify the data below, i.e. map each datum $x$ to a label $y$. Select tests to be as small as possible (in terms of attributes), breaking ties among tests with the same number of attributes by selecting the one that classifies the greatest number of examples correctly. If multiple tests have the same number of attributes and classify the same number of examples, then break the tie using attributes with lower index numbers (e.g. select $A_1$ over $A_2$)\n",
    "\n",
    "| Datum  | $A_1$ | $A_2$ | $A_3$ | $A_4$ |  y  |\n",
    "| -------| :---: | :---: | :---: | :---: | ---:|\n",
    "| $x_1$  |   1   |   0   |   0   |   0   |  1  |\n",
    "| $x_2$  |   1   |   0   |   1   |   1   |  1  |\n",
    "| $x_3$  |   0   |   1   |   0   |   0   |  1  |\n",
    "| $x_4$  |   0   |   1   |   1   |   0   |  0  |\n",
    "| $x_5$  |   1   |   1   |   0   |   1   |  1  |\n",
    "| $x_6$  |   0   |   1   |   0   |   1   |  0  |\n",
    "| $x_7$  |   0   |   0   |   1   |   1   |  1  |\n",
    "| $x_8$  |   0   |   0   |   1   |   0   |  0  |\n",
    "\n",
    "Hints: \n",
    "To get started, it might help to draw out the tree by hand with each attribute representing a node.\n",
    "\n",
    "To create the decision function to pass to your `DecisionNode`, you can create a lambda expression as follows:\n",
    "\n",
    "    func = lambda feature : feature[2] == 0\n",
    "    \n",
    "in which we would choose the left node if the third attribute is 0.\n",
    "\n",
    "The tree nodes should be less than 10 nodes including the leaf (not the number of instances, but the actual nodes in the tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examples = [[1,0,0,0],\n",
    "            [1,0,1,1],\n",
    "            [0,1,0,0],\n",
    "            [0,1,1,0],\n",
    "            [1,1,0,1],\n",
    "            [0,1,0,1],\n",
    "            [0,0,1,1],\n",
    "            [0,0,1,0]]\n",
    "\n",
    "classes = [1,1,1,0,1,0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_decision_tree():\n",
    "    \"\"\"Create decision tree\n",
    "    capable of handling the provided \n",
    "    data.\"\"\"\n",
    "    # TODO: build full tree from root\n",
    "    decision_tree_root = None\n",
    "    \n",
    "    return decision_tree_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = build_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1b: Validation\n",
    "--------\n",
    "5 pts.\n",
    "\n",
    "Now that we have a decision tree, we're going to need some way to evaluate its performance. In most cases we'd reserve a portion of the training data for evaluation, or use [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). For now let's just see how your tree does on the provided examples. In the stubbed out code below, fill out the methods to compute accuracy, precision, recall, and the confusion matrix for your classifier output. `classifier_output` is just the list of labels that your classifier outputs, corresponding to the same examples as `true_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(classifier_output, true_labels):\n",
    "    #TODO output should be [[true_positive, false_negative], [false_positive, true_negative]]\n",
    "    raise NotImplemented()\n",
    "\n",
    "def precision(classifier_output, true_labels):\n",
    "    #TODO precision is measured as: true_positive/ (true_positive + false_positive)\n",
    "    raise NotImplemented()\n",
    "    \n",
    "def recall(classifier_output, true_labels):\n",
    "    #TODO: recall is measured as: true_positive/ (true_positive + false_negative)\n",
    "    raise NotImplemented()\n",
    "    \n",
    "def accuracy(classifier_output, true_labels):\n",
    "    #TODO accuracy is measured as:  correct_classifications / total_number_examples\n",
    "    raise NotImplemented()\n",
    "    \n",
    "classifier_output = [decision_tree_root.decide(example) for example in examples]\n",
    "\n",
    "p1_accuracy = accuracy( classifier_output, classes )\n",
    "p1_precision = precision(classifier_output, classes)\n",
    "p1_recall = recall(classifier_output, classes)\n",
    "p1_confusion_matrix = confusion_matrix(classifier_output, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2a: Decision Tree Learning\n",
    "-------\n",
    "30 pts.\n",
    "\n",
    "As the size of our training set grows, it rapidly becomes impractical to build these trees by hand. We need a procedure to automagically construct these trees.\n",
    "\n",
    "For starters, let's consider the following algorithm (a variation of [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm)) for the construction of a decision tree from a given set of examples:\n",
    "\n",
    "    1) Check for base cases: \n",
    "         a) If all elements of a list are of the same class, return a leaf node with the appropriate class label.\n",
    "         b) If a specified depth limit is reached, return a leaf labeled with the most frequent class.\n",
    "\n",
    "    2) For each attribute alpha: evaluate the normalized information gain gained by splitting on attribute $\\alpha$\n",
    "\n",
    "    3) Let alpha_best be the attribute with the highest normalized information gain\n",
    "\n",
    "    4) Create a decision node that splits on alpha_best\n",
    "\n",
    "    5) Recur on the sublists obtained by splitting on alpha_best, and add those nodes as children of node\n",
    "\n",
    "First, in the `DecisionTree.__build_tree__()` method implement the above algorithm. You will need to implement `entropy()` and `information_gain()` in order to do so (hints [here](https://en.wikipedia.org/wiki/Entropy_(information_theory)) and [here](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)).\n",
    "\n",
    "Next, in `DecisionTree.classify()` below, write a function to produce classifications for a list of features once your decision tree has been built.\n",
    "\n",
    "Your features and classify should be in numpy arrays where if the dataset was (m x n) then the features is (m x n-1) and classify is (n x 1).\n",
    "\n",
    "You need at least an average accuracy of 60% to get full credit for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(class_vector):\n",
    "    \"\"\"Compute the entropy for a list\n",
    "    of classes (given as either 0 or 1).\"\"\"\n",
    "    # TODO: finish this\n",
    "    raise NotImplemented()\n",
    "    \n",
    "def information_gain(previous_classes, current_classes ):\n",
    "    \"\"\"Compute the information gain between the\n",
    "    previous and current classes (each \n",
    "    a list of 0 and 1 values).\"\"\"\n",
    "    # TODO: finish this\n",
    "    raise NotImplemented()\n",
    "\n",
    "class DecisionTree():\n",
    "    \"\"\"Class for automatic tree-building\n",
    "    and classification.\"\"\"\n",
    "\n",
    "    def __init__(self, depth_limit=float('inf')):\n",
    "        \"\"\"Create a decision tree with an empty root\n",
    "        and the specified depth limit.\"\"\"\n",
    "        self.root = None\n",
    "        self.depth_limit = depth_limit\n",
    "\n",
    "    def fit(self, features, classes):\n",
    "        \"\"\"Build the tree from root using __build_tree__().\"\"\"\n",
    "        self.root = self.__build_tree__(features, classes)\n",
    "\n",
    "    def __build_tree__(self, features, classes, depth=0):  \n",
    "        \"\"\"Implement the above algorithm to build\n",
    "        the decision tree using the given features and\n",
    "        classes to build the decision functions.\"\"\"\n",
    "        #TODO: finish this\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def classify(self, features):\n",
    "        \"\"\"Use the fitted tree to \n",
    "        classify a list of examples. \n",
    "        Return a list of class labels.\"\"\"\n",
    "        class_labels = []\n",
    "        #TODO: finish this\n",
    "        raise NotImplemented()\n",
    "        return class_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2b: Validation\n",
    "--------\n",
    "10 pts.\n",
    "\n",
    "For this part of the assignment we're going to use a relatively simple dataset (banknote authentication, found in `part_2_data.csv`). In the section below there are methods to load the data in a consistent format.\n",
    "\n",
    "In general, reserving part of your data as a test set can lead to unpredictable performance- a serendipitous choice of your train or test split could give you a very inaccurate idea of how your classifier performs. That's where k-fold cross validation comes in.\n",
    "\n",
    "In `generate_k_folds()`, we'll split the dataset at random into k equal subsections. Then iterating on each of our k samples, we'll reserve that sample for testing and use the other k-1 for training. Averaging the results of each fold should give us a more consistent idea of how the classifier is doing across the data as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv(data_file_path, class_index=-1):\n",
    "    handle = open(data_file_path, 'r')\n",
    "    contents = handle.read()\n",
    "    handle.close()\n",
    "    rows = contents.split('\\n')\n",
    "    out = np.array([[float(i) for i in r.split(',')] for r in rows if r ])\n",
    "    classes= map(int,  out[:, class_index])\n",
    "    features = out[:, :class_index]\n",
    "    return features, classes\n",
    "\n",
    "def generate_k_folds(dataset, k):\n",
    "    #TODO this method should return a list of folds,\n",
    "    # where each fold is a tuple like (training_set, test_set)\n",
    "    # where each set is a tuple like (examples, classes)\n",
    "    raise NotImplemented()\n",
    "    \n",
    "dataset = load_csv('part2_data.csv')\n",
    "ten_folds = generate_k_folds(dataset, 10)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "confusion = []\n",
    "\n",
    "for fold in ten_folds:\n",
    "    train, test = fold\n",
    "    train_features, train_classes = train\n",
    "    test_features, test_classes = test\n",
    "    tree = DecisionTree( )\n",
    "    tree.fit( train_features, train_classes)\n",
    "    output = tree.classify(test_features)\n",
    "    \n",
    "    accuracies.append( accuracy(output, test_classes))\n",
    "    precisions.append( precision(output, test_classes))\n",
    "    recalls.append( recall(output, test_classes))\n",
    "    confusion.append( confusion_matrix(output, test_classes))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Random Forests\n",
    "-------\n",
    "30 pts.\n",
    "\n",
    "The decision boundaries drawn by decision trees are very sharp, and fitting a decision tree of unbounded depth to a list of training examples almost inevitably leads to overfitting. In an attempt to decrease the variance of our classifier we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated 'bagging').\n",
    "\n",
    "A Random Forest is a collection of decision trees, build as follows:\n",
    "\n",
    "    1) For every tree we're going to build:\n",
    "\n",
    "        a) Subsample the examples provided us (with replacement) in accordance with a provided example subsampling rate.\n",
    "\n",
    "        b) From the sample in a), choose attributes at random to learn on (in accordance with a provided attribute subsampling rate)\n",
    "\n",
    "        c) Fit a decision tree to the subsample of data we've chosen (to a certain depth)\n",
    "    \n",
    "Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n",
    "Fill in `RandomForest.fit()` to fit the decision tree as we describe above, and fill in `RandomForest.classify()` to classify a given list of examples.\n",
    "\n",
    "Your features and classify should be in numpy arrays where if the dataset was (m x n) then the features is (m x n-1) and classify is (n x 1).\n",
    "\n",
    "You need at least an average accuracy of 75% to get full credit for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \"\"\"Class for random forest\n",
    "    classification.\"\"\"\n",
    "\n",
    "    def __init__(self, num_trees, depth_limit, example_subsample_rate, attr_subsample_rate):\n",
    "        \"\"\"Create a random forest with a fixed \n",
    "        number of trees, depth limit, example\n",
    "        sub-sample rate and attribute sub-sample\n",
    "        rate.\"\"\"\n",
    "        self.trees = []\n",
    "        self.num_trees = num_trees\n",
    "        self.depth_limit = depth_limit\n",
    "        self.example_subsample_rate = example_subsample_rate\n",
    "        self.attr_subsample_rate = attr_subsample_rate\n",
    "\n",
    "    def fit(self, features, classes):\n",
    "        \"\"\"Build a random forest of \n",
    "        decision trees.\"\"\"\n",
    "        # TODO implement the above algorithm\n",
    "        raise NotImplemented()\n",
    "\n",
    "    def classify(self, features):\n",
    "        \"\"\"Classify a list of features based\n",
    "        on the trained random forest.\"\"\"\n",
    "        # TODO implement classification for a random forest.\n",
    "        raise NotImplemented()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Challenge!\n",
    "-------\n",
    "\n",
    "You've been provided with a sample of data from a research dataset in 'challenge_data.pickle'. It is serialized as a tuple of (features, classes) and you will need to create a tree structure. I have reserved a part of the dataset for testing. The classifier that performs most accurately on the holdout set wins (so optimize for accuracy). To get full points for this part of the assignment, you'll need to get at least an average accuracy of 80% on the data you have (testing on training data), and at least an average accuracy of 60% on the holdout set (test data set).\n",
    "\n",
    "Ties will be broken by submission time.\n",
    "\n",
    "First place:  3 bonus points on your final grade\n",
    "\n",
    "Second place: 2 bonus points on your final grade\n",
    "\n",
    "Third place:  1 bonus point on your final grade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ChallengeClassifier():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # initialize whatever parameters you may need here-\n",
    "        # this method will be called without parameters \n",
    "        # so if you add any to make parameter sweeps easier, provide defaults\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def fit(self, features, classes):\n",
    "        # fit your model to the provided features\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def classify(self, features):\n",
    "        # classify each feature in features as either 0 or 1.\n",
    "        raise NotImplemented()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
